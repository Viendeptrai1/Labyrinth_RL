# Labyrinth RL - T√†i Li·ªáu Chi Ti·∫øt D·ª± √Ån

<div align="center">

**Multi-Floor 3D Maze Environment + Deep Reinforcement Learning**

*T·ª´ Q-Learning ƒë·∫øn SAC/PPO - A Complete RL Journey*

</div>

---

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan D·ª± √Ån](#1-t·ªïng-quan-d·ª±-√°n)
2. [Ki·∫øn Tr√∫c H·ªá Th·ªëng](#2-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
3. [C·∫•u Tr√∫c Th∆∞ M·ª•c](#3-c·∫•u-tr√∫c-th∆∞-m·ª•c)
4. [Flow Ho·∫°t ƒê·ªông](#4-flow-ho·∫°t-ƒë·ªông)
5. [MDP Formulation](#5-mdp-formulation)
6. [Components Chi Ti·∫øt](#6-components-chi-ti·∫øt)
7. [Level Design System](#7-level-design-system)
8. [Roadmap Ph√°t Tri·ªÉn](#8-roadmap-ph√°t-tri·ªÉn)
9. [H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng](#9-h∆∞·ªõng-d·∫´n-s·ª≠-d·ª•ng)

---

## 1. T·ªïng Quan D·ª± √Ån

### 1.1 Gi·ªõi Thi·ªáu

**Labyrinth RL** l√† m·ªôt d·ª± √°n h·ªçc t·∫≠p v√† nghi√™n c·ª©u v·ªÅ Reinforcement Learning, ƒë∆∞·ª£c x√¢y d·ª±ng theo phong c√°ch c·ªßa hai cu·ªën s√°ch kinh ƒëi·ªÉn:

- **"Reinforcement Learning: An Introduction" (Sutton & Barto)** - N·ªÅn t·∫£ng l√Ω thuy·∫øt MDP, Bellman Equations, Tabular Methods
- **"Deep Reinforcement Learning Hands-On" (Maxim Lapan)** - Tri·ªÉn khai th·ª±c t·∫ø v·ªõi PyTorch, Deep RL algorithms

### 1.2 M·ª•c Ti√™u D·ª± √Ån

| M·ª•c Ti√™u | M√¥ T·∫£ |
|----------|-------|
| **H·ªçc thu·∫≠t** | Hi·ªÉu s√¢u v·ªÅ RL t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao |
| **Th·ª±c h√†nh** | Tri·ªÉn khai c√°c thu·∫≠t to√°n RL state-of-the-art |
| **T√≠ch h·ª£p** | K·∫øt h·ª£p game engine v·ªõi RL training pipeline |
| **Curriculum Learning** | H·ªá th·ªëng levels t·ª´ d·ªÖ ƒë·∫øn kh√≥ cho agent h·ªçc |

### 1.3 Technology Stack

```
Backend:        Python 3.8+, PyTorch 2.0+, Gymnasium (OpenAI Gym)
Environment:    Custom Gym Environment (2D Physics)
Visualization:  Apache Zeppelin + AngularJS + Three.js
Training:       SAC (Soft Actor-Critic), TD3 (future), PPO (future)
Infrastructure: Docker + Docker Compose
```

---

## 2. Ki·∫øn Tr√∫c H·ªá Th·ªëng

### 2.1 S∆° ƒê·ªì T·ªïng Quan

```mermaid
graph TB
    subgraph "Frontend - Visualization"
        Z[Zeppelin Notebook]
        A[AngularJS UI]
        T[Three.js 3D Renderer]
    end
    
    subgraph "Backend - Python/PySpark"
        E[LabyrinthEnv<br/>Gymnasium Environment]
        B[ZeppelinBridge<br/>State Binding]
        W[World<br/>Physics & Entities]
    end
    
    subgraph "RL Training"
        AG[SAC Agent]
        BUF[Replay Buffer]
        TR[Training Loop]
    end
    
    subgraph "Data Layer"
        LEV[Level Specs<br/>JSON]
        MOD[Model Checkpoints]
        LOG[Training Logs]
    end
    
    Z --> B
    B --> E
    E --> W
    B --> A
    A --> T
    
    E --> AG
    AG --> BUF
    AG --> TR
    
    TR --> MOD
    TR --> LOG
    E --> LEV
    
    style E fill:#4A90E2,color:#fff
    style AG fill:#E24A4A,color:#fff
    style Z fill:#7B68EE,color:#fff
```

### 2.2 Lu·ªìng D·ªØ Li·ªáu (Data Flow)

```mermaid
sequenceDiagram
    participant U as User/Agent
    participant Z as Zeppelin UI
    participant B as Bridge
    participant E as Environment
    participant W as World
    participant P as Physics Engine
    
    U->>Z: Input Action (WASD / tilt)
    Z->>B: apply_tilt(pitch, roll)
    B->>E: step(action)
    E->>W: update(dt)
    W->>P: simulate physics
    P->>W: new positions
    W->>E: collision events
    E->>E: calculate reward
    E->>B: obs, reward, done, info
    B->>Z: z.angularBind(state)
    Z->>U: Update 3D Visualization
```

### 2.3 Design Patterns

| Pattern | ·ª®ng D·ª•ng | V·ªã Tr√≠ |
|---------|----------|--------|
| **Entity-Component** | Qu·∫£n l√Ω game objects (Ball, Wall, Coin, etc.) | `src/labyrinth_env/core/entity.py` |
| **Strategy** | ƒê·ªïi policy/reward d·ªÖ d√†ng | `src/rl/agents/base.py` |
| **State Machine** | Game states (Playing, Won, Lost) | `src/labyrinth_env/core/state_machine.py` |
| **Observer (Pub-Sub)** | Event system cho binding | `src/labyrinth_env/core/events.py` |
| **Factory + Registry** | T·∫°o entities t·ª´ level spec | `src/labyrinth_env/entities/builders.py` |
| **Command** | Input handling | `src/labyrinth_env/core/commands.py` |
| **Bridge** | K·∫øt n·ªëi backend-frontend | `src/labyrinth_env/bridge.py` |

---

## 3. C·∫•u Tr√∫c Th∆∞ M·ª•c

```
Labyrinth_RL/
‚îú‚îÄ‚îÄ üìÇ src/                          # Source code ch√≠nh
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ labyrinth_env/            # Game environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ core/                 # Core systems
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entity.py           # Base Entity class
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ world.py            # World (ch·ª©a entities)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components.py       # Position, Physics, Renderable
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ events.py           # EventBus, GameEvent
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ state_machine.py    # Game state management
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commands.py         # Input commands
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ entities/            # Game objects
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ builders.py         # Entity factories
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env.py                  # LabyrinthEnv (Gym API)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ level_spec.py           # Level loader & registry
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bridge.py               # Zeppelin binding
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ rl/                       # Reinforcement Learning
‚îÇ       ‚îú‚îÄ‚îÄ üìÇ agents/              
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ base.py             # PolicyStrategy interface
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ sac.py              # SAC Agent implementation
‚îÇ       ‚îú‚îÄ‚îÄ buffer.py               # Replay Buffer
‚îÇ       ‚îî‚îÄ‚îÄ train.py                # Training loop utilities
‚îú‚îÄ‚îÄ üìÇ notebooks/                    # Zeppelin notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_Labyrinth_Game_*.zpln    # Main game notebook
‚îÇ   ‚îú‚îÄ‚îÄ 01_labyrinth_game.json      # JSON export
‚îÇ   ‚îî‚îÄ‚îÄ 02_train_sac.json           # SAC training notebook
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ levels/                  # Level definitions (JSON)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ level_01_tutorial.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ level_02_simple_maze.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ level_10_master.json
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ models/                  # Trained checkpoints (.pth)
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ logs/                    # Training logs
‚îú‚îÄ‚îÄ üìÇ assets/                       # Frontend resources
‚îú‚îÄ‚îÄ Dockerfile                       # Zeppelin container
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### 3.1 Gi·∫£i Th√≠ch C√°c Module Ch√≠nh

#### **`src/labyrinth_env/`** - Game Environment

- **Core Systems** (`core/`):
  - `entity.py`: Base class cho t·∫•t c·∫£ game objects
  - `world.py`: Container ch·ª©a entities, qu·∫£n l√Ω physics loop
  - `events.py`: EventBus ƒë·ªÉ publish/subscribe events (coin collected, goal reached, etc.)
  - `state_machine.py`: State machine cho game flow (Playing ‚Üí Won/Lost)
  
- **Environment** (`env.py`):
  - Conform Gymnasium API: `reset()`, `step()`, `render()`, `close()`
  - Observation space: ball position, velocity, tilt angles, distances
  - Action space: continuous `[pitch, roll]` ho·∫∑c discrete 8-direction
  - Reward shaping: goal, coin, hole penalty, time penalty

- **Bridge** (`bridge.py`):
  - K·∫øt n·ªëi Python backend v·ªõi AngularJS frontend
  - S·ª≠ d·ª•ng `z.angularBind()` ƒë·ªÉ sync state
  - Event handlers t·ª± ƒë·ªông update UI khi c√≥ thay ƒë·ªïi

#### **`src/rl/`** - RL Algorithms

- **Agents**:
  - `base.py`: Interface `PolicyStrategy` cho t·∫•t c·∫£ agents
  - `sac.py`: **Soft Actor-Critic** (Haarnoja et al., 2018)
    - Twin Q-Networks (gi·∫£m overestimation)
    - Automatic entropy tuning
    - Continuous action space
  
- **Buffer** (`buffer.py`):
  - Replay Buffer v·ªõi prioritized sampling (optional)
  - Efficient numpy-based storage

---

## 4. Flow Ho·∫°t ƒê·ªông

### 4.1 Human Play Flow

```mermaid
flowchart LR
    A[User nh·∫•n WASD] --> B[Zeppelin AngularJS]
    B --> C[bridge.move_direction]
    C --> D[env.move_direction]
    D --> E[world.update x N steps]
    E --> F[physics simulation]
    F --> G{Check collisions}
    G -->|Goal| H[Episode Won]
    G -->|Hole| I[Episode Lost]
    G -->|Coin| J[Collect & Continue]
    G -->|None| K[Continue]
    K --> L[Return obs, reward]
    L --> M[z.angularBind state]
    M --> N[Update 3D View]
```

### 4.2 RL Training Flow

```mermaid
flowchart TB
    START([Start Training]) --> RESET[env.reset]
    RESET --> OBS[Get observation]
    OBS --> WARMUP{Steps < warmup?}
    WARMUP -->|Yes| RANDOM[Random action]
    WARMUP -->|No| POLICY[agent.select_action]
    
    RANDOM --> STEP
    POLICY --> STEP[env.step action]
    STEP --> STORE[buffer.push transition]
    STORE --> UPDATE{Steps % update_every == 0?}
    
    UPDATE -->|Yes| SAMPLE[Sample batch]
    SAMPLE --> COMPUTE[Compute Q-loss, policy-loss]
    COMPUTE --> BACKWARD[Backpropagation]
    BACKWARD --> SOFT[Soft update targets]
    SOFT --> CHECK
    
    UPDATE -->|No| CHECK{Done?}
    CHECK -->|No| OBS
    CHECK -->|Yes| LOG[Log episode metrics]
    LOG --> SAVE{Episode % save_freq == 0?}
    SAVE -->|Yes| CHECKPOINT[Save model]
    SAVE -->|No| RESET
    CHECKPOINT --> RESET
```

### 4.3 Level Progression Flow

```mermaid
stateDiagram-v2
    [*] --> Level01_Tutorial
    Level01_Tutorial --> Level02_SimpleMaze: Win + criteria met
    Level02_SimpleMaze --> Level03_Holes: Win
    Level03_Holes --> Level04_KeyLock: Win
    Level04_KeyLock --> Level05_Teleport: Win
    Level05_Teleport --> Level06_MazeHoles: Win
    Level06_MazeHoles --> Level07_MultiKey: Win
    Level07_MultiKey --> Level08_TeleportMaze: Win
    Level08_TeleportMaze --> Level09_Gauntlet: Win
    Level09_Gauntlet --> Level10_Master: Win
    Level10_Master --> [*]: Master Completed
    
    note right of Level01_Tutorial
        Curriculum Learning:
        - Difficulty tƒÉng d·∫ßn
        - M·ªói level d·∫°y 1 mechanic m·ªõi
        - Agent ph·∫£i th√†nh th·∫°o level tr∆∞·ªõc
          m·ªõi chuy·ªÉn ti·∫øp
    end note
```

---

## 5. MDP Formulation

### 5.1 ƒê·ªãnh Nghƒ©a MDP

Theo **Sutton & Barto Chapter 3**, Labyrinth environment l√† m·ªôt **finite-horizon MDP**:

$$
\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma, T \rangle
$$

| K√Ω Hi·ªáu | T√™n | M√¥ T·∫£ |
|---------|-----|-------|
| $\mathcal{S}$ | State Space | Continuous (position, velocity, tilt, distances) |
| $\mathcal{A}$ | Action Space | Continuous $[-1, 1]^2$ ho·∫∑c Discrete $\{0,‚Ä¶,8\}$ |
| $\mathcal{P}$ | Transition Dynamics | Deterministic physics (c√≥ th·ªÉ th√™m noise) |
| $\mathcal{R}$ | Reward Function | Shaped reward (goal + coin - hole - time) |
| $\gamma$ | Discount Factor | 0.99 (default) |
| $T$ | Max Horizon | 3000 steps (configurable) |

### 5.2 State Space $\mathcal{S}$

**Observation vector** (dimension = 2 + 2 + 2 + N):

```python
obs = [
    ball.x,         # Ball position X
    ball.y,         # Ball position Y
    ball.vx,        # Ball velocity X (optional)
    ball.vy,        # Ball velocity Y (optional)
    tilt.pitch,     # Board tilt pitch (optional)
    tilt.roll,      # Board tilt roll (optional)
    # Distances to objects (optional, N varies)
    dist_to_goal,
    dist_to_nearest_hole,
    dist_to_nearest_coin,
    ...
]
```

**Normalization**: T·∫•t c·∫£ values ƒë∆∞·ª£c normalize v·ªÅ $[-1, 1]$ ho·∫∑c $[0, 1]$ ƒë·ªÉ gi√∫p neural network h·ªçc t·ªët h∆°n.

### 5.3 Action Space $\mathcal{A}$

#### Mode 1: Continuous (cho SAC/TD3)

$$
a_t = [\text{pitch}, \text{roll}] \in [-1, 1]^2
$$

- ƒê∆∞·ª£c scale l√™n th√†nh g√≥c nghi√™ng th·ª±c t·∫ø: $[-0.15, 0.15]$ radians

#### Mode 2: Discrete 8-Direction (cho DQN)

$$
a_t \in \{0, 1, 2, ‚Ä¶, 8\}
$$

```
0 = none      1 = up        2 = upright
3 = right     4 = downright 5 = down
6 = downleft  7 = left      8 = upleft
```

### 5.4 Reward Function $\mathcal{R}$

**Composite reward** (theo Sutton & Barto Chapter 6 - Shaping Functions):

$$
r_t = r_{\text{goal}} + r_{\text{coin}} + r_{\text{hole}} + r_{\text{time}} + r_{\text{distance}}
$$

| Component | Value | ƒêi·ªÅu Ki·ªán |
|-----------|-------|-----------|
| $r_{\text{goal}}$ | +1000 | Ball ch·∫°m goal |
| $r_{\text{coin}}$ | +100 √ó scale | Ball thu coin |
| $r_{\text{hole}}$ | -100 | Ball r∆°i v√†o hole |
| $r_{\text{time}}$ | -0.1 | M·ªói step (encourage speed) |
| $r_{\text{distance}}$ | $\Delta d \times$ scale | Gi·∫£m kho·∫£ng c√°ch ƒë·∫øn goal (optional) |

**Reward Shaping**: Theo Ng et al. (1999), potential-based shaping kh√¥ng l√†m thay ƒë·ªïi optimal policy n·∫øu tu√¢n th·ªß:

$$
r'(s,a,s') = r(s,a,s') + \gamma \Phi(s') - \Phi(s)
$$

v·ªõi $\Phi(s) = -\text{distance\_to\_goal}(s)$.

### 5.5 Bellman Optimality Equation

M·ª•c ti√™u training l√† t√¨m optimal Q-function (Sutton & Barto Chapter 4):

$$
Q^*(s,a) = \mathbb{E}\left[r + \gamma \max_{a'} Q^*(s', a')\right]
$$

SAC m·ªü r·ªông v·ªõi **maximum entropy objective** (Haarnoja et al., 2018):

$$
\pi^* = \arg\max_\pi \mathbb{E}_{\tau \sim \pi}\left[\sum_t r_t + \alpha \mathcal{H}(\pi(\cdot|s_t))\right]
$$

---

## 6. Components Chi Ti·∫øt

### 6.1 LabyrinthEnv (`src/labyrinth_env/env.py`)

**Class ch√≠nh** conform Gymnasium API:

```python
class LabyrinthEnv:
    def __init__(self, config: EnvConfig = None, levels_dir: str = None)
    def reset(self, level_id: str = None, seed: int = None) -> Tuple[np.ndarray, Dict]
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]
    def render(self, mode: str = 'state') -> Dict
    def close(self)
```

**Observation Space**:
```python
spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
```

**Action Space**:
```python
spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)  # Continuous
# OR
spaces.Discrete(9)  # Discrete 8-dir
```

**Key Methods**:

- `move_direction(direction: str, num_steps: int)`: Di chuy·ªÉn ball theo h∆∞·ªõng v·ªõi momentum (hybrid control)
- `step_discrete_8dir(action: int)`: Wrapper cho discrete actions
- `_calculate_reward(step_info)`: T√≠nh composite reward
- `_get_observation()`: Build observation vector v·ªõi normalization

### 6.2 World & Physics (`src/labyrinth_env/core/world.py`)

**World** l√† physics container:

```python
class World:
    def __init__(self, width: float, height: float, max_tilt: float = 0.15)
    def update(self, dt: float) -> Dict[str, Any]
    def add_entity(self, entity: Entity)
    def remove_entity(self, entity: Entity)
    def emit_event(self, event_type: GameEvent, data: Dict)
```

**Physics Loop** (trong `update()`):

1. T√≠nh gravity force t·ª´ tilt: $F_g = mg \sin(\theta)$
2. Apply forces l√™n ball entities
3. Update velocities & positions
4. Check collisions (ball-wall, ball-hole, ball-coin, ball-goal)
5. Emit events cho collision handlers

### 6.3 SAC Agent (`src/rl/agents/sac.py`)

**Network Architecture**:

```python
# Actor: Gaussian Policy
state ‚Üí MLP(256, 256) ‚Üí [mean, log_std] ‚Üí Normal(Œº, œÉ) ‚Üí action

# Twin Critics: Q1, Q2
[state, action] ‚Üí MLP(256, 256) ‚Üí Q-value
```

**Update Rules** (theo paper Haarnoja et al., 2018):

**Critic Update**:
$$
L_Q = \mathbb{E}\left[(Q(s,a) - (r + \gamma \min_{i=1,2} Q_{\text{target}}(s', a') - \alpha \log \pi(a'|s')))^2\right]
$$

**Actor Update**:
$$
L_\pi = \mathbb{E}\left[\alpha \log \pi(a|s) - Q(s,a)\right]
$$

**Temperature Update** (auto-tuning):
$$
L_\alpha = -\mathbb{E}\left[\alpha (\log \pi(a|s) + \mathcal{H}_{\text{target}})\right]
$$

**Soft Target Update**:
$$
\theta_{\text{target}} \leftarrow \tau \theta + (1-\tau) \theta_{\text{target}}
$$

### 6.4 Zeppelin Bridge (`src/labyrinth_env/bridge.py`)

**Binding Workflow**:

```python
class ZeppelinBridge:
    def __init__(self, zeppelin_context, config):
        self.z = zeppelin_context  # Zeppelin notebook context
        self.env = LabyrinthEnv(config)
        
    def step(self, action):
        obs, reward, done, truncated, info = self.env.step(action)
        state = self.env.get_state()
        self._bind('gameState', state)  # Auto-update AngularJS
        return {'obs': obs, 'reward': reward, 'done': done, 'state': state}
    
    def _bind(self, name, value):
        self.z.angularBind(name, json.dumps(value, cls=NumpyEncoder))
```

**Event Handlers**:
- `_on_coin_collected`: Update score, trigger animation
- `_on_episode_ended`: Show victory/defeat screen
- `_on_state_updated`: Refresh 3D visualization

---

## 7. Level Design System

### 7.1 Level Spec Format (JSON)

```json
{
  "id": "level_01_tutorial",
  "name": "Tutorial",
  "difficulty": 1,
  "description": "ƒê∆∞a bi v√†o l·ªó. D√πng WASD ƒë·ªÉ nghi√™ng b√†n.",
  "hints": ["Nghi√™ng b√†n sang ph·∫£i ƒë·ªÉ bi lƒÉn v·ªÅ goal"],
  
  "board": {
    "width": 8.0,
    "height": 8.0,
    "max_tilt": 0.15
  },
  
  "ball": {
    "start": [-3.0, 0.0],
    "radius": 0.3
  },
  
  "goal": {
    "position": [3.0, 0.0],
    "radius": 0.6,
    "reward": 1000.0
  },
  
  "walls": [
    {"position": [0, -3], "size": [4.0, 0.5, 0.2]},
    {"position": [0, 3], "size": [4.0, 0.5, 0.2]}
  ],
  
  "holes": [],
  "coins": [],
  "keys": [],
  "locks": [],
  "teleports": [],
  
  "time_limit": 0,
  
  "rewards": {
    "goal": 1000.0,
    "hole_penalty": -100.0,
    "coin_value": 100,
    "time_penalty": -0.1
  }
}
```

### 7.2 Curriculum Levels

| Level | Name | Mechanics | Difficulty |
|-------|------|-----------|------------|
| 01 | Tutorial | Basic tilt movement | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ |
| 02 | Simple Maze | Walls navigation | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ |
| 03 | Holes | Avoid traps | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ |
| 04 | Key & Lock | Unlock doors | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ |
| 05 | Teleport | Portals | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ |
| 06 | Maze + Holes | Combination | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ |
| 07 | Multi-Key | Multiple objectives | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ |
| 08 | Teleport Maze | Complex navigation | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ |
| 09 | Gauntlet | High precision | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ |
| 10 | Master | All mechanics | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ |

**Level Loader** (`src/labyrinth_env/level_spec.py`):

```python
class LevelLoader:
    @staticmethod
    def load_level(filepath: str) -> LevelSpec
    
    @staticmethod
    def get_builtin_levels() -> List[LevelSpec]
    
    @staticmethod
    def register_level(level_spec: LevelSpec)
```

---

## 8. Roadmap Ph√°t Tri·ªÉn

### 8.1 Giai ƒêo·∫°n Hi·ªán T·∫°i ‚úÖ

- [x] Custom Gymnasium Environment (2D)
- [x] Physics simulation (tilt-based gravity)
- [x] Entity-Component architecture
- [x] Level system v·ªõi 10 levels
- [x] Zeppelin integration v·ªõi AngularJS binding
- [x] SAC agent implementation
- [x] Replay Buffer
- [x] Training loop c∆° b·∫£n

### 8.2 Giai ƒêo·∫°n Ti·∫øp Theo üöß

#### **Phase 1: Menu & Game Loop Enhancement**

> [!IMPORTANT]
> M·ª•c ti√™u: T·∫°o tr·∫£i nghi·ªám ch∆°i game ho√†n ch·ªânh v·ªõi menu, level progression, v√† save system

**A. Main Menu System**

```python
class MenuSystem:
    """
    Game menu v·ªõi c√°c options:
    - New Game / Continue
    - Select Level
    - Settings (difficulty, controls)
    - View Leaderboard
    - Watch Agent Play
    """
    
    states = ['MAIN_MENU', 'LEVEL_SELECT', 'PLAYING', 'PAUSED', 'GAME_OVER', 'VICTORY']
```

**Features c·∫ßn implement**:

- [ ] Main menu UI trong AngularJS
- [ ] Level selection screen (hi·ªÉn th·ªã locked/unlocked levels)
- [ ] Save/Load game progress (JSON file)
- [ ] Pause menu (resume, restart, quit to menu)
- [ ] Victory screen (stats, next level, replay)
- [ ] Game Over screen (retry, return to menu)

**UI Mock (AngularJS + Three.js)**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        üéÆ LABYRINTH RL              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [‚ñ∂ New Game]                       ‚îÇ
‚îÇ  [üíæ Continue]                      ‚îÇ
‚îÇ  [üìã Select Level]                  ‚îÇ
‚îÇ  [ü§ñ Watch Agent Play]              ‚îÇ
‚îÇ  [‚öôÔ∏è  Settings]                     ‚îÇ
‚îÇ  [üèÜ Leaderboard]                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**B. Level Progression System**

```python
class ProgressionManager:
    """
    Qu·∫£n l√Ω ti·∫øn ƒë·ªô qua levels:
    - Unlock criteria (stars, time, score)
    - Star rating system (1-3 stars)
    - Track best times & scores
    """
    
    def check_level_unlock(current_level: str, stats: Dict) -> bool:
        """Check if player can unlock next level"""
        pass
    
    def calculate_stars(level_id: str, time: float, score: int) -> int:
        """Calculate star rating (1-3)"""
        pass
```

**Unlock Criteria**:

| Level | Unlock Requirement |
|-------|--------------------|
| 02 | Complete Level 01 |
| 03 | Complete Level 02 v·ªõi ‚â•2 stars |
| 04-10 | Complete previous level |

**C. Agent Demonstration Mode**

```python
class DemoMode:
    """
    Agent t·ª± ch∆°i ƒë·ªÉ demo trained policy
    """
    
    def run_agent_demo(level_id: str, agent_path: str):
        """
        Load trained agent v√† play level
        - Slow-motion mode ƒë·ªÉ xem r√µ
        - Show action/Q-values overlay
        - Restart n·∫øu failed
        """
        pass
```

**Flow Diagram - Menu System**:

```mermaid
stateDiagram-v2
    [*] --> MainMenu
    MainMenu --> LevelSelect: Select Level
    MainMenu --> Playing: New Game
    MainMenu --> AgentDemo: Watch Agent
    
    LevelSelect --> Playing: Choose Level
    
    Playing --> Paused: Press ESC
    Playing --> Victory: Reach Goal
    Playing --> GameOver: Fall in Hole
    
    Paused --> Playing: Resume
    Paused --> MainMenu: Quit
    
    Victory --> NextLevel: Continue
    Victory --> LevelSelect: Level Select
    Victory --> MainMenu: Main Menu
    
    GameOver --> Playing: Retry
    GameOver --> MainMenu: Quit
    
    AgentDemo --> MainMenu: Stop
    
    NextLevel --> Playing
```

#### **Phase 2: Advanced RL Algorithms**

> [!NOTE]
> M·ªü r·ªông t·ª´ SAC sang c√°c thu·∫≠t to√°n kh√°c

**A. Tabular Methods (cho small discrete version)**

- [ ] Value Iteration (Sutton & Barto Chapter 4)
- [ ] Policy Iteration
- [ ] Q-Learning (Chapter 6.5)
- [ ] SARSA (Chapter 6.4)
- [ ] Expected SARSA

**Visualization**: Heatmap c·ªßa Q-values cho t·ª´ng state

**B. Deep RL - Value-based**

- [ ] DQN (Mnih et al., 2015)
  - Experience Replay
  - Target Network
  - Epsilon-greedy exploration
- [ ] Double DQN (van Hasselt et al., 2015)
- [ ] Dueling DQN (Wang et al., 2016)
- [ ] Rainbow DQN (Hessel et al., 2017)

**C. Deep RL - Policy Gradient**

- [ ] REINFORCE (Sutton & Barto Chapter 13)
- [ ] A2C (Advantage Actor-Critic)
- [ ] PPO (Proximal Policy Optimization)
  - Theo Lapan Chapter 11
  - Clipped objective
  - GAE (Generalized Advantage Estimation)

**D. Multi-Agent RL (Future)**

- [ ] Competitive mode: 2 agents race
- [ ] Cooperative mode: 2 agents c√πng collect items

#### **Phase 3: 3D Multi-Floor Maze**

> [!CAUTION]
> ƒê√¢y l√† major upgrade, c·∫ßn refactor environment

**3D Extensions**:

- [ ] Z-axis physics (gravity, vertical velocity)
- [ ] Stairs / Ramps gi·ªØa c√°c t·∫ßng
- [ ] Camera control (free look, follow ball)
- [ ] 3D collision detection
- [ ] Fog of war (ch·ªâ th·∫•y current floor)

**MDP Changes**:

- State space th√™m `ball.z`, `current_floor`
- Action space c√≥ th·ªÉ th√™m `jump` action
- Reward cho vertical exploration

#### **Phase 4: Advanced Features**

**A. Curriculum Learning**

- [ ] Auto-adjust difficulty d·ª±a tr√™n agent performance
- [ ] Procedural level generation
- [ ] Transfer learning gi·ªØa c√°c levels

**B. Explainability / Visualization**

- [ ] Saliency maps (which part of state matters?)
- [ ] Reward decomposition chart
- [ ] Trajectory replay v·ªõi slow-motion
- [ ] Q-value heatmap overlay tr√™n game

**C. Imitation Learning**

- [ ] Record human play traces
- [ ] Behavioral Cloning (BC)
- [ ] DAgger (Dataset Aggregation)
- [ ] Inverse RL (learn reward t·ª´ human demos)

**D. Robustness**

- [ ] Domain randomization (noise in physics)
- [ ] Adversarial levels (hard-generated levels)
- [ ] Test generalization tr√™n unseen levels

---

## 9. H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng

### 9.1 C√†i ƒê·∫∑t & Ch·∫°y

#### Option 1: Docker (Recommended)

```bash
# Clone repository
git clone <repo-url>
cd Labyrinth_RL

# Build v√† start Zeppelin
docker-compose up --build

# Truy c·∫≠p Zeppelin UI
open http://localhost:8080
```

#### Option 2: Local

```bash
# T·∫°o virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Run as Gym environment
python -c "from src.labyrinth_env import LabyrinthEnv; env = LabyrinthEnv(); env.reset()"
```

### 9.2 Human Play (Zeppelin Notebook)

1. M·ªü notebook `01_Labyrinth_Game_*.zpln`
2. Ch·∫°y paragraph `%pyspark` ƒë·ªÉ init environment
3. Ch·∫°y paragraph `%angular` ƒë·ªÉ render game
4. D√πng **WASD** ho·∫∑c **Arrow Keys** ƒë·ªÉ ƒëi·ªÅu khi·ªÉn
5. M·ª•c ti√™u: ƒê∆∞a bi (üîµ) v√†o goal (üéØ)

### 9.3 Train RL Agent (SAC)

```python
from src.labyrinth_env import LabyrinthEnv, EnvConfig
from src.rl.agents.sac import SACAgent, SACConfig

# Create environment
env = LabyrinthEnv(config=EnvConfig(max_steps=3000))

# Create agent
agent = SACAgent(
    state_dim=env.observation_dim(),
    action_dim=env.action_dim(),
    config=SACConfig(
        hidden_dim=256,
        actor_lr=3e-4,
        batch_size=256,
        buffer_size=100000
    )
)

# Training loop
for episode in range(1000):
    obs, info = env.reset()
    done = False
    episode_reward = 0
    
    while not done:
        action = agent.select_action(obs, deterministic=False)
        next_obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        agent.store_transition(obs, action, reward, next_obs, done)
        
        if agent.step_count > agent.config.warmup_steps:
            metrics = agent.update()
        
        obs = next_obs
        episode_reward += reward
    
    print(f"Episode {episode}: Reward = {episode_reward:.2f}")
    
    # Save checkpoint
    if episode % 100 == 0:
        agent.save(f"data/models/sac_ep{episode}.pth")
```

### 9.4 Evaluate Agent

```python
# Load trained agent
agent.load("data/models/sac_ep1000.pth")
agent.eval_mode()

# Run evaluation
for level_id in env.get_available_levels():
    obs, info = env.reset(level_id=level_id)
    done = False
    total_reward = 0
    
    while not done:
        action = agent.select_action(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        total_reward += reward
    
    print(f"{level_id}: {'SUCCESS' if info.get('success') else 'FAILED'}, Reward={total_reward}")
```

---

## 10. T√≥m T·∫Øt & Next Steps

### ‚úÖ ƒê√£ Ho√†n Th√†nh

- **Environment**: Gymnasium-compatible Labyrinth 2D v·ªõi physics simulation
- **RL Agent**: SAC implementation v·ªõi Twin Q-Networks
- **Integration**: Zeppelin bridge cho human play & visualization
- **Levels**: 10 curriculum levels t·ª´ tutorial ƒë·∫øn master

### üéØ B∆∞·ªõc Ti·∫øp Theo (Theo Th·ª© T·ª± ∆Øu Ti√™n)

1. **Menu System** (Phase 1A)
   - Main menu, level select, pause menu
   - Save/Load progress
   - Victory/GameOver screens

2. **Agent Demo Mode** (Phase 1C)
   - Load trained agent v√† demo play
   - Overlay action/Q-value visualization

3. **More RL Algorithms** (Phase 2)
   - DQN, PPO ƒë·ªÉ so s√°nh performance
   - Visualize learning curves

4. **3D Multi-Floor** (Phase 3)
   - Long-term goal: M·ªü r·ªông sang 3D environment

---

## üìö References

### Books

1. **Sutton, R. S., & Barto, A. G.** (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
   - Chapters 3-6: MDP, Bellman Equations, Tabular Methods
   - Chapter 13: Policy Gradient

2. **Lapan, M.** (2020). *Deep Reinforcement Learning Hands-On* (2nd ed.). Packt Publishing.
   - Chapter 6: DQN
   - Chapter 14: SAC

### Papers

1. **Haarnoja, T., et al.** (2018). "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor." *ICML*.

2. **Mnih, V., et al.** (2015). "Human-level control through deep reinforcement learning." *Nature*.

3. **Schulman, J., et al.** (2017). "Proximal Policy Optimization Algorithms." *arXiv:1707.06347*.

---

<div align="center">

**Happy Learning! üöÄü§ñ**

*"The only way to do great work is to love what you do." - Steve Jobs*

</div>
